{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Statistics Advance - 2"
      ],
      "metadata": {
        "id": "BpseISknJD3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is hypothesis testing in statistics?\n",
        " - Hypothesis testing is a fundamental statistical technique that helps researchers and analysts draw conclusions about a population based on sample data. It begins with forming a null hypothesis (H₀) that represents a default assumption, and an alternative hypothesis (H₁) that proposes an opposing perspective. The process involves collecting data, applying statistical tests, and interpreting the results to determine whether the evidence supports or contradicts the null hypothesis. Hypothesis testing is widely used across scientific research, business analytics, and decision-making processes to validate claims or predictions."
      ],
      "metadata": {
        "id": "CFxeRgBuJHer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the null hypothesis, and how does it differ from the alternative hypothesis?\n",
        " - The null hypothesis (H₀) represents the assumption that there is no effect, difference, or relationship in the population, serving as the baseline for comparison. The alternative hypothesis (H₁), on the other hand, contradicts H₀ by suggesting that there is a significant effect or difference. Researchers aim to collect evidence to reject or fail to reject the null hypothesis, depending on the outcomes of statistical testing.\n",
        "\n",
        " Example: In testing a new medication:\n",
        "\n",
        " Null Hypothesis (H₀): \"The medication has no effect on blood pressure.\"\n",
        "\n",
        " Alternative Hypothesis (H₁): \"The medication significantly reduces blood pressure.\"\n",
        "\n",
        " The null hypothesis represents the status quo, while the alternative hypothesis aligns with the research objective."
      ],
      "metadata": {
        "id": "uWeLEyirJWGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the significance level in hypothesis testing, and why is it important?\n",
        "  - The significance level, denoted as α, is the threshold for determining whether the null hypothesis should be rejected. It reflects the probability of making a Type I error—rejecting H₀ when it is true. Common significance levels include 0.05 (5%) and 0.01 (1%).\n",
        "\n",
        "    Importance:\n",
        "\n",
        "    The significance level controls the likelihood of false positives, ensuring that the conclusions drawn from the test are reliable.\n",
        "\n",
        "    Setting an appropriate α value balances the trade-off between Type I and Type II errors, depending on the context of the study. For instance, in medical research, a lower significance level (e.g., α = 0.01) may be used to minimize the risk of incorrectly concluding that a treatment is effective."
      ],
      "metadata": {
        "id": "Wo4C6kZTJuf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does a P-value represent in hypothesis testing?\n",
        " - The P-value quantifies the probability of observing results as extreme as, or more extreme than, those in the data, assuming the null hypothesis is true. It provides a measure of how incompatible the data is with H₀ and is used to decide whether there is sufficient evidence to reject H₀.\n",
        "\n",
        "  A low P-value indicates strong evidence against the null hypothesis, while a high P-value suggests that the data is consistent with H₀. For example, a P-value of 0.02 means there is a 2% chance of obtaining results as extreme as the observed data, assuming H₀ is true."
      ],
      "metadata": {
        "id": "DJ57W-UTJ5iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do you interpret the P-value in hypothesis testing?\n",
        " - If the P-value is less than or equal to the significance level (α), we reject the null hypothesis and accept that the alternative hypothesis is supported by the evidence.\n",
        "\n",
        "    If the P-value is greater than the significance level, we fail to reject the null hypothesis, meaning there is insufficient evidence to support the alternative hypothesis.\n",
        "\n",
        "    Example: If a study testing a new diet yields a P-value of 0.04 and the significance level is α = 0.05, we reject H₀, concluding that the diet has a statistically significant effect. However, if the P-value were 0.10, we would fail to reject H₀, as the evidence would not be strong enough."
      ],
      "metadata": {
        "id": "YzREz2rcKTIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What are Type 1 and Type 2 errors in hypothesis testing?\n",
        " - Type 1 Error (False Positive): This error occurs when the null hypothesis (H₀) is rejected even though it is actually true. In other words, it is concluding that there is an effect or difference when none actually exists. The probability of making a Type I error is equal to the significance level (α), such as 5% (0.05).\n",
        "\n",
        "    Example: A medical test incorrectly indicates that a patient has a disease when they do not.\n",
        "\n",
        "    Type 2 Error (False Negative): This error occurs when the null hypothesis is not rejected even though it is actually false. Essentially, it is failing to detect an effect or difference when one does exist. The probability of making a Type 2 error is denoted by β, and it decreases as sample size increases.\n",
        "\n",
        "    Example: A medical test fails to detect a disease in a patient who actually has it.\n",
        "\n",
        "    These errors highlight the trade-offs in hypothesis testing: reducing α (Type 1 error) often increases β (Type 2 error), so balancing these probabilities is critical in study design."
      ],
      "metadata": {
        "id": "g90p3J1_Ko5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the difference between a one-tailed and a two-tailed test in hypothesis testing?\n",
        " - One-Tailed Test: This type of test examines whether a parameter is either greater than or less than a specified value in a specific direction. Researchers use it when they are only interested in deviations in one direction.\n",
        "\n",
        "    Example: Testing if a new drug increases the average recovery rate compared to an existing drug. The null hypothesis (H₀) might state that the new drug’s recovery rate is the same or lower, while the alternative hypothesis (H₁) states it is higher.\n",
        "\n",
        "    Two-Tailed Test: This test examines whether a parameter differs from a specified value in either direction. It is used when deviations in both directions (higher or lower) are of interest.\n",
        "\n",
        "    Example: Testing whether a new teaching method results in a different average test score (either higher or lower) compared to the traditional method. The null hypothesis (H₀) would state the scores are the same, while the alternative hypothesis (H₁) states they are different.\n",
        "\n",
        "    The choice between one-tailed and two-tailed tests depends on the research question, with two-tailed tests being more conservative as they account for effects in both directions."
      ],
      "metadata": {
        "id": "0NZvkSk1LfAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Z-test, and when is it used in hypothesis testing?\n",
        " - The Z-test is a statistical method used to determine whether there is a significant difference between the sample mean and the population mean, or between two sample means. The test assumes that the data is normally distributed, and it requires the population standard deviation to be known.\n",
        "\n",
        "    When to use a Z-test:\n",
        "\n",
        "    When the sample size is large (n > 30).\n",
        "\n",
        "    When the population standard deviation is known.\n",
        "\n",
        "    Example: Suppose the average test score of students nationwide is 75, with a population standard deviation of 10. A random sample of 50 students has an average score of 78. We can use a Z-test to determine if the sample mean is significantly different from the population mean."
      ],
      "metadata": {
        "id": "QCnYIB8_LsI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How do you calculate the Z-score, and what does it represent in hypothesis testing?\n",
        "  - Z-Score Formula: $$ Z = \\frac{\\text{(Sample Mean - Population Mean)}}{\\text{(Population Standard Deviation / √Sample Size)}} $$\n",
        "\n",
        "    Interpretation of the Z-Score: The Z-score represents how many standard deviations the sample mean is from the population mean. A higher absolute value of the Z-score indicates that the sample mean is further away from the population mean.\n",
        "\n",
        "    Example: If the sample mean is 78, the population mean is 75, the population standard deviation is 10, and the sample size is 50: $$ Z = \\frac{78 - 75}{10 / \\sqrt{50}} = \\frac{3}{1.41} \\approx 2.13 $$\n",
        "\n",
        "    A Z-score of 2.13 indicates that the sample mean is 2.13 standard deviations above the population mean. If this Z-score corresponds to a P-value less than the significance level (e.g., α = 0.05), we reject the null hypothesis."
      ],
      "metadata": {
        "id": "FAAW0N-3L49p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the T-distribution, and when should it be used instead of the normal distribution?\n",
        " - The T-distribution is a probability distribution that accounts for uncertainty in the estimation of the population standard deviation. It is similar to the normal distribution but has thicker tails, which means it accounts for more variability, particularly in smaller samples.\n",
        "\n",
        "     When to use the T-distribution:\n",
        "\n",
        "     When the sample size is small (n < 30).\n",
        "\n",
        "     When the population standard deviation is unknown.\n",
        "\n",
        "     Example: Suppose we want to determine whether a new training program improves average employee performance. We collect data from a small group of 15 employees, but the population standard deviation is unknown. In this case, we use the T-distribution for hypothesis testing.\n",
        "\n",
        "     The T-distribution becomes nearly identical to the normal distribution as the sample size increases, which is why Z-tests are used for larger samples."
      ],
      "metadata": {
        "id": "8DSJJA0hMcso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the difference between a Z-test and a T-test?\n",
        " - While both Z-tests and T-tests are used to compare sample means, they differ in their assumptions and applications:\n",
        "\n",
        "  Z-Test:\n",
        "\n",
        "   Used when the population standard deviation is known.\n",
        "\n",
        "   Suitable for large sample sizes (n > 30).\n",
        "\n",
        "   Relies on the standard normal distribution.\n",
        "\n",
        "   T-Test:\n",
        "\n",
        "   Used when the population standard deviation is unknown.\n",
        "\n",
        "   Suitable for small sample sizes (n < 30).\n",
        "\n",
        "   Uses the T-distribution, which adjusts for the added uncertainty in small samples.\n",
        "\n",
        "   Example Comparison: If a study involves a sample size of 100 and the population standard deviation is known, a Z-test is appropriate. If the sample size is 20 and the population standard deviation is unknown, a T-test is used instead."
      ],
      "metadata": {
        "id": "sBLdbT4OMvKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the T-test, and how is it used in hypothesis testing?\n",
        " - The T-test is a statistical test that determines whether there is a significant difference between the means of groups or a sample and the population. It accounts for variability in small samples and situations where the population standard deviation is unknown.\n",
        "\n",
        "   Types of T-tests:\n",
        "\n",
        "   One-Sample T-Test: Compares a sample mean to a known population mean.\n",
        "\n",
        "   Independent (Two-Sample) T-Test: Compares the means of two independent groups.\n",
        "\n",
        "   Paired (Dependent) T-Test: Compares means from the same group at different times (e.g., before and after treatment).\n",
        "\n",
        "   Example: A company wants to test whether employee productivity has increased after implementing a new policy. They measure productivity levels before and after the policy for 10 employees and use a paired T-test to analyze the data."
      ],
      "metadata": {
        "id": "jsC_92pJ1Aa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the relationship between Z-test and T-test in hypothesis testing?\n",
        " - The Z-test and T-test are both methods for evaluating hypotheses regarding the means of populations or samples, but their usage differs based on the available data and sample size.\n",
        "\n",
        "     Z-Test: This test is applicable when the population standard deviation is known and the sample size is large (n > 30). It assumes a normal distribution and provides precise results due to the availability of population parameters.\n",
        "\n",
        "     T-Test: This test is used when the population standard deviation is unknown and must be estimated from the sample. It is particularly suitable for small samples (n < 30) and relies on the T-distribution, which adjusts for added variability.\n",
        "\n",
        "     Relationship: As the sample size increases, the T-distribution approaches the normal (Z) distribution. For large samples, the distinction between Z-test and T-test becomes negligible, as both yield similar results. In small samples, the T-test provides a more conservative and accurate assessment.\n",
        "\n",
        "     Example Comparison: If we are testing whether the average weight of adults in a city is 70 kg, and:\n",
        "\n",
        "     We have a sample size of 50 people and know the population standard deviation: Use a Z-test.\n",
        "\n",
        "     We have a sample size of 20 people and do not know the population standard deviation: Use a T-test."
      ],
      "metadata": {
        "id": "I4-aU-tw1rIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is a confidence interval, and how is it used to interpret statistical results?\n",
        " - A confidence interval is a range of values, calculated from sample data, that is likely to contain the true population parameter with a specified level of confidence (e.g., 95%, 99%). It quantifies the uncertainty in the estimate of the parameter.\n",
        "\n",
        "  Formula for a Confidence Interval (CI): $$ \\text{CI} = \\text{Sample Mean} \\pm \\text{(Critical Value)} \\cdot \\text{(Standard Error)} $$\n",
        "\n",
        "   How to Use It in Interpretation:\n",
        "\n",
        "If the confidence interval does not include the null hypothesis value, we reject the null hypothesis.\n",
        "\n",
        "The width of the interval depends on the sample size, variability in the data, and the confidence level. A narrower interval suggests more precision in estimating the parameter.\n",
        "\n",
        "Example: In a study of average test scores, a 95% confidence interval for the mean is calculated as (72, 78). This means we are 95% confident that the true population mean lies between 72 and 78. If the null hypothesis states that the mean is 80, we reject H₀ because 80 falls outside the interval."
      ],
      "metadata": {
        "id": "UUETOMYyTejY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is the margin of error, and how does it affect the confidence interval?\n",
        " - The margin of error is the maximum amount by which the sample estimate is expected to differ from the true population parameter. It reflects the uncertainty in the estimate and depends on factors such as sample size, variability, and confidence level.\n",
        "\n",
        "Formula: $$ \\text{Margin of Error (ME)} = \\text{(Critical Value)} \\cdot \\text{(Standard Error)} $$\n",
        "\n",
        "Effect on Confidence Interval:\n",
        "\n",
        "A larger margin of error results in a wider confidence interval, indicating greater uncertainty.\n",
        "\n",
        "A smaller margin of error narrows the confidence interval, suggesting more precision in the estimate.\n",
        "\n",
        "Example: Suppose a survey estimates that 60% of people prefer a product, with a margin of error of ±5%. This means the true percentage is likely between 55% and 65%. A smaller margin of error would produce a tighter range, increasing confidence in the exact estimate."
      ],
      "metadata": {
        "id": "MqiDLhZhUxym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How is Bayes' Theorem used in statistics, and what is its significance?\n",
        " - Bayes' Theorem is a mathematical formula used to calculate conditional probabilities. It provides a framework for updating prior beliefs based on new evidence and is integral to Bayesian statistics.\n",
        "\n",
        "Formula: $$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
        "\n",
        "Significance: Bayes' Theorem allows researchers to:\n",
        "\n",
        "Revise probabilities as new data becomes available.\n",
        "\n",
        "Apply probabilistic reasoning in uncertain scenarios, such as medical diagnostics, spam filtering, or risk assessment.\n",
        "\n",
        "Example: Suppose 1% of people in a population have a disease (P(A)). A test correctly identifies the disease 95% of the time (P(B|A)) and has a false positive rate of 5% (P(B|¬A)). Using Bayes' Theorem, we can calculate the probability that a person who tested positive actually has the disease (P(A|B))."
      ],
      "metadata": {
        "id": "nrPVDtOMVCHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is the Chi-square distribution, and when is it used?\n",
        " - The Chi-square distribution is a probability distribution used to test hypotheses about categorical data. It is non-symmetric and skewed to the right, with its shape determined by the degrees of freedom.\n",
        "\n",
        "Uses:\n",
        "\n",
        "Testing for independence between two categorical variables in a contingency table.\n",
        "\n",
        "Evaluating how well observed frequencies match expected frequencies (goodness of fit).\n",
        "\n",
        "Example: A researcher studies whether gender (male/female) and preference for a product (yes/no) are independent. They collect data in a contingency table and use the Chi-square test to analyze the association."
      ],
      "metadata": {
        "id": "_COmOoxpVJL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the Chi-square goodness of fit test, and how is it applied?\n",
        " - The Chi-square goodness of fit test evaluates whether observed frequencies in a sample match expected frequencies from a theoretical distribution.\n",
        "\n",
        "Steps to Apply:\n",
        "\n",
        "* Define the null hypothesis (H₀): The observed data follows the expected distribution.\n",
        "\n",
        "* Calculate the Chi-square statistic: $$ \\chi^2 = \\sum \\frac{(\\text{Observed} - \\text{Expected})^2}{\\text{Expected}} $$\n",
        "\n",
        "* Compare the calculated statistic to the critical value from the Chi-square table for the given degrees of freedom.\n",
        "\n",
        "Example: Suppose a dice is rolled 60 times, and the observed frequencies for each face are compared to the expected frequency (10 rolls per face) using the Chi-square goodness of fit test."
      ],
      "metadata": {
        "id": "4HQagy12VNXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What is the F-distribution, and when is it used in hypothesis testing?\n",
        " - The F-distribution is a probability distribution used to compare variances between two or more groups. It is asymmetric and skewed to the right, with its shape determined by two sets of degrees of freedom.\n",
        "\n",
        "Uses:\n",
        "\n",
        "In ANOVA (Analysis of Variance) to test whether group means are significantly different.\n",
        "\n",
        "In regression analysis to evaluate the overall fit of a model.\n",
        "\n",
        "Example: A researcher compares the variances in test scores between three classrooms. They use the F-distribution to test if the variances are significantly different."
      ],
      "metadata": {
        "id": "RvXj6HnnVTYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is an ANOVA test, and what are its assumptions?\n",
        " - ANOVA (Analysis of Variance) is a statistical method used to determine whether there are significant differences between the means of three or more groups. It uses the F-distribution to analyze the variance within and between groups.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "Observations are independent.\n",
        "\n",
        "Data in each group is normally distributed.\n",
        "\n",
        "Variances are equal across groups (homogeneity of variance).\n",
        "\n",
        "Example: A company evaluates the effectiveness of three training programs on employee performance. Using ANOVA, they test whether the mean performance scores differ across the groups."
      ],
      "metadata": {
        "id": "DCrnZomVVmJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What are the different types of ANOVA tests?\n",
        " - Analysis of Variance (ANOVA) comes in various forms, depending on the number of factors being analyzed and the relationships among the groups. Here are the common types:\n",
        "\n",
        "     One-Way ANOVA:\n",
        "\n",
        "     This tests the differences among the means of three or more independent groups, based on a single factor (independent variable).\n",
        "\n",
        "     Example: A researcher wants to determine if three different diets result in differing average weight loss. The independent variable is \"diet type,\" and the dependent variable is \"weight loss.\"\n",
        "\n",
        "     If the ANOVA test shows significant differences, post-hoc tests (e.g., Tukey's HSD) can be conducted to identify which groups differ.\n",
        "\n",
        "     Two-Way ANOVA:\n",
        "\n",
        "     This evaluates the effect of two independent variables on a dependent variable, as well as any interaction between the two factors.\n",
        "\n",
        "     Example: A company tests two training programs (Factor A) and two different training durations (Factor B) to assess the effect on employee performance. Two-Way ANOVA can determine if the type of training, the duration, or their interaction influences performance.\n",
        "\n",
        "     Repeated Measures ANOVA:\n",
        "\n",
        "     Used when the same subjects are measured multiple times under different conditions or over time.\n",
        "\n",
        "     Example: A psychologist measures the stress levels of participants before therapy, mid-treatment, and after therapy. Repeated measures ANOVA analyzes how stress changes over time.\n",
        "\n",
        "     These types of ANOVA are essential tools in experimental designs, as they allow for testing multiple variables and their interactions efficiently."
      ],
      "metadata": {
        "id": "uY02K3K5Vq5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What is the F-test, and how does it relate to hypothesis testing?\n",
        " - The F-test is a statistical test used to compare the variances of two datasets or the variances between groups in ANOVA. It evaluates whether the observed differences in variability are greater than what could be expected by chance.\n",
        "\n",
        "Formula for the F-statistic: $$ F = \\frac{\\text{Variance Between Groups}}{\\text{Variance Within Groups}} $$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "A higher F-value indicates that the differences between group means are significant relative to the variability within the groups.\n",
        "\n",
        "The F-statistic is compared to a critical value from the F-distribution table, based on the degrees of freedom for both numerator (between-group variability) and denominator (within-group variability).\n",
        "\n",
        "Example in ANOVA: Suppose a researcher analyzes the test scores of students from three different teaching methods. If the F-statistic exceeds the critical value at a significance level (e.g., α = 0.05), the null hypothesis (that all group means are equal) is rejected. This means that at least one group mean differs significantly from the others."
      ],
      "metadata": {
        "id": "nFFXgf-rW7hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical question"
      ],
      "metadata": {
        "id": "Un5HI8sJXF6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and interpret the results.\n",
        "\n",
        " - To conduct a Z-test:\n",
        "\n",
        "* Define the null hypothesis (H₀) and alternative hypothesis (H₁).\n",
        "\n",
        "* Use the formula for the Z-test: $$ Z = \\frac{\\text{(Sample Mean - Population Mean)}}{\\text{(Standard Deviation / √Sample Size)}} $$\n",
        "\n",
        "* Calculate the Z-score based on the sample data.\n",
        "\n",
        "* Compare the Z-score to the critical value from the Z-table corresponding to your significance level (α).\n",
        "\n",
        "* Interpret the results: Reject or fail to reject H₀."
      ],
      "metadata": {
        "id": "Ni8tSCaYXLA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Sample data\n",
        "sample_mean = 78\n",
        "population_mean = 75\n",
        "population_std_dev = 10\n",
        "sample_size = 50\n",
        "\n",
        "# Calculate Z-score\n",
        "z_score = (sample_mean - population_mean) / (population_std_dev / (sample_size ** 0.5))\n",
        "\n",
        "# Calculate P-value\n",
        "p_value = stats.norm.sf(abs(z_score)) * 2  # Two-tailed test\n",
        "\n",
        "# Output results\n",
        "print(f\"Z-score: {z_score}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "fVz6tbOKYkHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python.\n",
        " - Use Python's libraries to generate random data and test hypotheses. For example:\n",
        "\n",
        "* Simulate random data using numpy.random.\n",
        "\n",
        "* Perform hypothesis testing using scipy.stats."
      ],
      "metadata": {
        "id": "1AYTDY8oYsdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Simulate random data\n",
        "np.random.seed(42)  # Set seed for reproducibility\n",
        "data = np.random.normal(loc=75, scale=10, size=50)\n",
        "\n",
        "# Perform one-sample t-test\n",
        "t_stat, p_value = stats.ttest_1samp(data, 75)\n",
        "\n",
        "# Output results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "D4CSrCemZFco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement a one-sample Z-test using Python to compare the sample mean with the population mean."
      ],
      "metadata": {
        "id": "RMuDfSZ3ZKz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Given data\n",
        "population_mean = 75\n",
        "sample_mean = 78\n",
        "std_dev = 10  # Population standard deviation\n",
        "sample_size = 50\n",
        "\n",
        "# Calculate Z-score\n",
        "z_score = (sample_mean - population_mean) / (std_dev / (sample_size ** 0.5))\n",
        "\n",
        "# Calculate P-value for a two-tailed test\n",
        "p_value = stats.norm.sf(abs(z_score)) * 2\n",
        "\n",
        "# Output the results\n",
        "print(f\"Z-score: {z_score:.2f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Decision\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis (H₀). The sample mean significantly differs from the population mean.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis (H₀). There is insufficient evidence to suggest a difference.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "GDrxPZ5QcDrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Perform a two-tailed Z-test using Python and visualize the decision region on a plot.\n",
        " - After calculating the Z-score and P-value:\n",
        "\n",
        "i) Define the critical Z-values for a given significance level (e.g., ±1.96 for α = 0.05 in a two-tailed test).\n",
        "\n",
        "ii) Visualize the decision regions using Python's matplotlib."
      ],
      "metadata": {
        "id": "VM-oAOjzcHbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Critical Z-values for α = 0.05 (two-tailed)\n",
        "critical_value = stats.norm.ppf(1 - 0.05 / 2)\n",
        "\n",
        "# Generate Z-scores for visualization\n",
        "z_values = np.linspace(-3, 3, 1000)\n",
        "pdf = stats.norm.pdf(z_values)\n",
        "\n",
        "# Plot decision regions\n",
        "plt.plot(z_values, pdf, label='Normal Distribution')\n",
        "plt.axvline(x=critical_value, color='red', linestyle='--', label=f'Critical Value (+{critical_value:.2f})')\n",
        "plt.axvline(x=-critical_value, color='red', linestyle='--', label=f'Critical Value (-{critical_value:.2f})')\n",
        "\n",
        "plt.title('Decision Regions for Two-Tailed Z-Test')\n",
        "plt.xlabel('Z-score')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "rtKXut2bci04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing.\n",
        " - To visualize errors, simulate data under both the null hypothesis and the alternative hypothesis:\n",
        "\n",
        "* Type I error: Rejecting H₀ when it's true.\n",
        "\n",
        "* type II error: Failing to reject H₀ when it's false."
      ],
      "metadata": {
        "id": "dRhfXjWucxze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def visualize_errors(alpha, beta, mu_null, mu_alt, sigma):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    from scipy.stats import norm\n",
        "\n",
        "    x = np.linspace(mu_null - 3 * sigma, mu_alt + 3 * sigma, 1000)\n",
        "\n",
        "    # Distribution under null hypothesis (H₀)\n",
        "    y_null = norm.pdf(x, mu_null, sigma)\n",
        "    plt.plot(x, y_null, label='Null Hypothesis (H₀)', color='blue')\n",
        "\n",
        "    # Distribution under alternative hypothesis (H₁)\n",
        "    y_alt = norm.pdf(x, mu_alt, sigma)\n",
        "    plt.plot(x, y_alt, label='Alternative Hypothesis (H₁)', color='orange')\n",
        "\n",
        "    # Mark rejection region for Type I error\n",
        "    critical_value = norm.ppf(1 - alpha)\n",
        "    plt.axvline(critical_value, color='red', linestyle='--', label='Critical Value')\n",
        "\n",
        "    # Add labels and legend\n",
        "    plt.title(\"Type I and Type II Errors\")\n",
        "    plt.xlabel(\"Value\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "visualize_errors(alpha=0.05, beta=0.2, mu_null=75, mu_alt=78, sigma=10)\n",
        "'''"
      ],
      "metadata": {
        "id": "_0x7rWDYdGpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to perform an independent T-test and interpret the results.\n",
        " - An independent T-test compares means of two separate groups. Use scipy.stats.ttest_ind for the test."
      ],
      "metadata": {
        "id": "uk_pwFlUdJ0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Sample data\n",
        "group1 = [78, 75, 80, 79, 81]\n",
        "group2 = [85, 82, 88, 84, 89]\n",
        "\n",
        "# Perform T-test\n",
        "t_stat, p_value = stats.ttest_ind(group1, group2)\n",
        "\n",
        "# Output results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "RUKly0RhdT80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Perform a paired sample T-test using Python and visualize the comparison results.\n",
        " - A paired sample T-test is used when two related samples (e.g., measurements before and after treatment) are compared."
      ],
      "metadata": {
        "id": "as5uYmEtdW8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Sample data (Before and After)\n",
        "before = [78, 75, 80, 79, 81]\n",
        "after = [85, 82, 88, 84, 89]\n",
        "\n",
        "# Perform paired T-test\n",
        "t_stat, p_value = stats.ttest_rel(before, after)\n",
        "\n",
        "# Output results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "rvlxDc3VdaN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Simulate data and perform both Z-test and T-test, then compare the results using Python.\n",
        " - Explanation: For a small sample size with unknown population standard deviation, the T-test is more reliable. The Z-test is suitable when the population standard deviation is known."
      ],
      "metadata": {
        "id": "7W8bqPRKdqhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import numpy as np\n",
        "\n",
        "# Simulated data\n",
        "np.random.seed(42)\n",
        "sample = np.random.normal(loc=75, scale=10, size=25)  # Small sample (use T-Test)\n",
        "population_mean = 75\n",
        "population_std_dev = 10\n",
        "\n",
        "# Z-Test\n",
        "z_score = (np.mean(sample) - population_mean) / (population_std_dev / np.sqrt(len(sample)))\n",
        "z_p_value = stats.norm.sf(abs(z_score)) * 2\n",
        "\n",
        "# T-Test\n",
        "t_stat, t_p_value = stats.ttest_1samp(sample, population_mean)\n",
        "\n",
        "# Results\n",
        "print(\"Z-Test:\")\n",
        "print(f\"  Z-score: {z_score:.2f}, P-value: {z_p_value:.4f}\")\n",
        "print(\"T-Test:\")\n",
        "print(f\"  T-statistic: {t_stat:.2f}, P-value: {t_p_value:.4f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "Yhc_rk9geE3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python function to calculate the confidence interval for a sample mean and explain its significance.\n",
        " - Confidence intervals quantify uncertainty in estimating population parameters."
      ],
      "metadata": {
        "id": "47517ubReIzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def calculate_confidence_interval(sample_mean, sample_std, sample_size, confidence_level):\n",
        "    import scipy.stats as stats\n",
        "    critical_value = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1)\n",
        "    margin_of_error = critical_value * (sample_std / (sample_size ** 0.5))\n",
        "    lower_bound = sample_mean - margin_of_error\n",
        "    upper_bound = sample_mean + margin_of_error\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Example usage\n",
        "confidence_interval = calculate_confidence_interval(78, 10, 50, 0.95)\n",
        "print(f\"95% Confidence Interval: {confidence_interval}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "eHmUlz71ekC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to calculate the margin of error for a given confidence level using sample data.\n",
        " - Calculates the maximum expected difference between the sample mean and the true population mean for a specific confidence level."
      ],
      "metadata": {
        "id": "xIs23W51enPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from scipy.stats import norm\n",
        "\n",
        "def margin_of_error(sample_std, sample_size, confidence_level):\n",
        "    z = norm.ppf((1 + confidence_level) / 2)\n",
        "    error = z * (sample_std / (sample_size ** 0.5))\n",
        "    return error\n",
        "\n",
        "print(\"Margin of Error:\", margin_of_error(10, 50, 0.95))\n",
        "'''"
      ],
      "metadata": {
        "id": "OYMfeWw-fYPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process.\n",
        " - Bayes' Theorem updates probabilities based on prior knowledge and new evidence."
      ],
      "metadata": {
        "id": "KA2yVtlDfbIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def bayesian_inference(prior, likelihood, marginal):\n",
        "    posterior = (likelihood * prior) / marginal\n",
        "    return posterior\n",
        "\n",
        "prior = 0.01\n",
        "likelihood = 0.95\n",
        "marginal = (likelihood * prior) + (0.05 * 0.99)\n",
        "print(\"Posterior Probability:\", bayesian_inference(prior, likelihood, marginal))\n",
        "'''"
      ],
      "metadata": {
        "id": "xpzSRYaQfms-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Perform a Chi-square test for independence between two categorical variables in Python.\n",
        " - Tests whether two categorical variables are independent."
      ],
      "metadata": {
        "id": "Y47a_U1_fpqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "data = [[50, 30], [20, 40]]\n",
        "chi2, p, dof, expected = stats.chi2_contingency(data)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2}, P-value: {p}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "ttNkTxs7fyko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to calculate the expected frequencies for a Chi-square test based on observed data."
      ],
      "metadata": {
        "id": "QJ8S4-zFf1gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Observed data\n",
        "observed = [[50, 30], [20, 40]]\n",
        "\n",
        "# Expected frequencies from stats.chi2_contingency\n",
        "_, _, _, expected = stats.chi2_contingency(observed)\n",
        "\n",
        "print(\"Expected Frequencies:\")\n",
        "print(expected)\n",
        "'''"
      ],
      "metadata": {
        "id": "ue-rVppjgDtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution."
      ],
      "metadata": {
        "id": "Kf60NAnKgGCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "observed = [18, 22, 25, 20, 15]\n",
        "expected = [20, 20, 20, 20, 20]\n",
        "chi2, p = stats.chisquare(observed, expected)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2}, P-value: {p}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "KNAZOxvqgFnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics."
      ],
      "metadata": {
        "id": "l3dL-vHmgXGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = 4  # degrees of freedom\n",
        "x = np.linspace(0, 20, 1000)\n",
        "y = stats.chi2.pdf(x, df)\n",
        "\n",
        "plt.plot(x, y, label=f\"Chi-Square (df={df})\")\n",
        "plt.title(\"Chi-Square Distribution\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "RfHTcGtigWz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Implement an F-test using Python to compare the variances of two random samples."
      ],
      "metadata": {
        "id": "wHPAnBxKgbxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "group1 = [10, 20, 30, 40]\n",
        "group2 = [15, 25, 35, 45]\n",
        "f_stat, p_value = stats.f_oneway(group1, group2)\n",
        "\n",
        "print(f\"F-Statistic: {f_stat}, P-value: {p_value}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "ygQArRtoghRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to perform an ANOVA test to compare means between multiple groups and interpret the results."
      ],
      "metadata": {
        "id": "GZ3REUhNgh5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "group1 = [10, 20, 30]\n",
        "group2 = [15, 25, 35]\n",
        "group3 = [20, 30, 40]\n",
        "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
        "\n",
        "print(f\"F-Statistic: {f_stat}, P-value: {p_value}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "fulxmQP-gqyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Perform a one-way ANOVA test using Python to compare the means of different groups and plot the results."
      ],
      "metadata": {
        "id": "vFNwNBkcgrTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import seaborn as sns\n",
        "\n",
        "data = {'Group': ['A', 'A', 'B', 'B', 'C', 'C'], 'Scores': [10, 20, 15, 25, 20, 30]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "sns.boxplot(x='Group', y='Scores', data=df)\n",
        "plt.title(\"One-Way ANOVA Boxplot\")\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "YSgu_wgFhFGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVA."
      ],
      "metadata": {
        "id": "_GXWNVaChFfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from scipy.stats import shapiro, levene\n",
        "\n",
        "# Normality Test\n",
        "stat, p = shapiro(group1)\n",
        "print(\"Normality Test P-value:\", p)\n",
        "\n",
        "# Equal Variance Test\n",
        "stat, p = levene(group1, group2, group3)\n",
        "print(\"Equal Variance Test P-value:\", p)\n",
        "'''"
      ],
      "metadata": {
        "id": "aE47dGQqhKOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the results."
      ],
      "metadata": {
        "id": "mgJIhdHuhKoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Factor1': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
        "    'Factor2': ['X', 'Y', 'X', 'Y', 'X', 'Y'],\n",
        "    'Scores': [10, 15, 20, 25, 30, 35]\n",
        "})\n",
        "\n",
        "model = ols('Scores ~ Factor1 + Factor2 + Factor1:Factor2', data=df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(anova_table)\n",
        "'''"
      ],
      "metadata": {
        "id": "CZimSOzBhPMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing."
      ],
      "metadata": {
        "id": "buZSLEu7hPzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "d1, d2 = 5, 10\n",
        "x = np.linspace(0, 5, 1000)\n",
        "y = stats.f.pdf(x, d1, d2)\n",
        "\n",
        "plt.plot(x, y, label=f\"F-Distribution (df1={d1}, df2={d2})\")\n",
        "plt.title(\"F-Distribution\")\n",
        "plt.xlabel(\"F-Value\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "UtRQbDLehU4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group means."
      ],
      "metadata": {
        "id": "Dh5XwSnShVhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Data for three groups\n",
        "group1 = [78, 75, 80, 79, 81]\n",
        "group2 = [85, 82, 88, 84, 89]\n",
        "group3 = [90, 92, 95, 91, 93]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
        "\n",
        "# Results\n",
        "print(f\"F-Statistic: {f_stat:.2f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Decision\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. At least one group mean is significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. The group means are not significantly different.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "uL1p44kRhbkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means."
      ],
      "metadata": {
        "id": "Zmuk3Bl2hb_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def z_test_proportions(p1, p2, n1, n2):\n",
        "    p_hat = (p1 * n1 + p2 * n2) / (n1 + n2)\n",
        "    z_stat = (p1 - p2) / np.sqrt(p_hat * (1 - p_hat) * (1/n1 + 1/n2))\n",
        "    return z_stat\n",
        "\n",
        "p1, p2, n1, n2 = 0.6, 0.5, 100, 80\n",
        "z_stat = z_test_proportions(p1, p2, n1, n2)\n",
        "\n",
        "print(\"\\nQuestion 23:\")\n",
        "print(\"Z-Statistic:\", z_stat)\n",
        "'''"
      ],
      "metadata": {
        "id": "YKkOYrHyhg6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Perform a hypothesis test for population variance using a Chi-square distribution and interpret the results."
      ],
      "metadata": {
        "id": "SmLctOV_hhZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from scipy.stats import f\n",
        "\n",
        "data1 = np.random.normal(loc=10, scale=3, size=100)\n",
        "data2 = np.random.normal(loc=10, scale=2, size=100)\n",
        "\n",
        "f_stat = np.var(data1, ddof=1) / np.var(data2, ddof=1)\n",
        "df1, df2 = len(data1) - 1, len(data2) - 1\n",
        "p_value = f.sf(f_stat, df1, df2)\n",
        "\n",
        "print(\"\\nQuestion 24:\")\n",
        "print(\"F-Statistic:\", f_stat)\n",
        "print(\"P-Value:\", p_value)\n",
        "'''"
      ],
      "metadata": {
        "id": "Ms_wVpHIhmlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python script to perform a Z-test for comparing proportions between two datasets or groups."
      ],
      "metadata": {
        "id": "RxI6zNPthm_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import numpy as np\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Step 1: Define the data for two groups\n",
        "# Let's assume we have the following data:\n",
        "# Group 1: 50 successes out of 100 trials\n",
        "# Group 2: 30 successes out of 100 trials\n",
        "successes = np.array([50, 30])  # Number of successes in each group\n",
        "n = np.array([100, 100])        # Total trials in each group\n",
        "\n",
        "# Step 2: Perform the Z-test\n",
        "z_stat, p_value = proportions_ztest(count=successes, nobs=n)\n",
        "\n",
        "# Step 3: Analyze the results\n",
        "print(f\"Z-statistic: {z_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"We reject the null hypothesis: The proportions between the two groups are significantly different.\")\n",
        "else:\n",
        "    print(\"We fail to reject the null hypothesis: The proportions between the two groups are not significantly different.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "JGULRWwxhtU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Implement an F-test for comparing the variances of two datasets, then interpret and visualize the results."
      ],
      "metadata": {
        "id": "wmWGd0hJhttz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import f\n",
        "\n",
        "# Step 1: Generate two datasets\n",
        "np.random.seed(42)\n",
        "data1 = np.random.normal(loc=50, scale=5, size=100)  # Mean=50, Std=5\n",
        "data2 = np.random.normal(loc=50, scale=10, size=100) # Mean=50, Std=10\n",
        "\n",
        "# Step 2: Compute the variances\n",
        "var1 = np.var(data1, ddof=1)  # Variance of dataset 1\n",
        "var2 = np.var(data2, ddof=1)  # Variance of dataset 2\n",
        "\n",
        "# Step 3: Perform the F-test\n",
        "f_statistic = var1 / var2  # F-statistic\n",
        "df1 = len(data1) - 1       # Degrees of freedom for dataset 1\n",
        "df2 = len(data2) - 1       # Degrees of freedom for dataset 2\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 2 * min(f.cdf(f_statistic, df1, df2), 1 - f.cdf(f_statistic, df1, df2))\n",
        "\n",
        "# Step 4: Visualize the datasets\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(data1, bins=15, alpha=0.7, label='Dataset 1 (Variance = {:.2f})'.format(var1))\n",
        "plt.hist(data2, bins=15, alpha=0.7, label='Dataset 2 (Variance = {:.2f})'.format(var2))\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Comparison of Two Datasets')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Interpretation\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"We reject the null hypothesis: The variances of the two datasets are significantly different.\")\n",
        "else:\n",
        "    print(\"We fail to reject the null hypothesis: The variances of the two datasets are not significantly different.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "Lh06zLQOh0k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Perform a Chi-square test for goodness of fit with simulated data and analyze the results.\n",
        " - Explanation:\n",
        "Observed Data: This represents the frequencies of outcomes you observed in your experiment.\n",
        "\n",
        "Expected Data: This is what you would expect if the null hypothesis (e.g., the die is fair) is true.\n",
        "\n",
        "Chi-Square Test: The chisquare function from scipy.stats computes the test statistic and the p-value.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "A small p-value (e.g., < 0.05) indicates that the observed data is significantly different from the expected data.\n",
        "\n",
        "A large p-value indicates that there is no significant difference between the observed and expected data."
      ],
      "metadata": {
        "id": "xaUrb8BOh0_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import numpy as np\n",
        "from scipy.stats import chisquare\n",
        "\n",
        "# Step 1: Simulate the observed data\n",
        "# Let's assume we have a dice roll experiment (6-sided die)\n",
        "# Observed frequencies (e.g., results from 60 rolls of a die)\n",
        "observed = np.array([10, 9, 8, 12, 11, 10])\n",
        "\n",
        "# Step 2: Define the expected data (assuming a fair die)\n",
        "# Each face should have an equal probability (1/6), so expected frequency\n",
        "# will be total rolls divided by number of sides\n",
        "expected = np.array([10, 10, 10, 10, 10, 10])\n",
        "\n",
        "# Step 3: Perform the Chi-square test\n",
        "chi_statistic, p_value = chisquare(f_obs=observed, f_exp=expected)\n",
        "\n",
        "# Step 4: Analyze the results\n",
        "print(f\"Chi-Square Statistic: {chi_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"We reject the null hypothesis: The observed data does not fit the expected distribution.\")\n",
        "else:\n",
        "    print(\"We fail to reject the null hypothesis: The observed data fits the expected distribution.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "eqi4SCbQh5B0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}